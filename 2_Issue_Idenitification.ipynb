{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73fecd9e-bfd1-4e9d-96ab-9bc1328d73c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ETL: Metrics Calculation and Issue idenitification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37e1a628-08ae-47e9-b26e-5e372a7cd0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook uses SQL and Pyspark to identify issues based on extracted reivew aspects from [1_Review_Apspect_Extraction_Agent](/Workspace/Users/cindy.wu@databricks.com/voc_industry_demo/1_Review_Apspect_Extraction_Agent).\n",
    "\n",
    "Data Flow: \n",
    "raw reviews -> **review aspect extractions -> location aspect daily -> flag all issues**-> issue diagnosis and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ec0240-f170-4f41-9419-1507e2e023fb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761101085172}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    " *\n",
    "FROM lakehouse_inn_catalog.voc.review_extractions\n",
    "LIMIT 200;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229d58d7-3485-4594-bf69-93640924f341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RAW_ASPECTS   = \"lakehouse_inn_catalog.voc.review_aspect_details\"   # review_id, aspect, sentiment, evidence, opinion_terms\n",
    "RAW_REVIEWS   = \"lakehouse_inn_catalog.voc.raw_reviews\"              # review_id, location, review_date, review_text\n",
    "ASPECTS_DAILY = \"lakehouse_inn_catalog.voc.loc_aspect_daily\"         # location, aspect, date, neg_share_7d, volume_7d, delta_pp\n",
    "ISSUES_DAILY      = \"lakehouse_inn_catalog.voc.issues_daily\"\n",
    "RAW_EXTRACTS = \"lakehouse_inn_catalog.voc.review_extractions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "335ae1f1-8c8d-4976-8a64-801303ff7fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_date_param = \"2025-10-15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ccc5cbc-9015-4ec5-89e4-0375cfe0e2df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explode extraction by aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b652e68a-b404-4711-890b-5870cfda1f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`review_aspect_details` Table Schema\n",
    "\n",
    "Each row contains one aspect\n",
    "| **Column** | **Type** | **Description** |\n",
    "|-------------|-----------|-----------------|\n",
    "| `review_id` | `STRING` | Unique review identifier. |\n",
    "| `aspect` | `STRING` | Aspect extracted from the review (e.g., `cleanliness`, `staff_friendliness`). |\n",
    "| `sentiment` | `STRING` | Sentiment toward the aspect (`very_negative`, `negative`, `neutral`, `positive`, `very_positive`). |\n",
    "| `evidence` | `ARRAY<STRING>` | Text snippets supporting the sentiment classification. |\n",
    "| `opinion_terms` | `ARRAY<STRING>` | Opinion words or short phrases describing the aspect (e.g., \"dirty\", \"friendly\"). |\n",
    "| `location` | `STRING` | City or region where the hotel is located. |\n",
    "| `latitude` | `DECIMAL(6,4)` | Latitude coordinate of the hotel. |\n",
    "| `longitude` | `DECIMAL(7,4)` | Longitude coordinate of the hotel. |\n",
    "| `channel` | `STRING` | Source channel where the review was posted (e.g., Google, TripAdvisor). |\n",
    "| `star_rating` | `LONG` | Numeric star rating provided by the customer. |\n",
    "| `review_date` | `DATE` | Date when the review was posted. |\n",
    "| `review_text` | `STRING` | Full text of the customer review. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ec369c-67ac-458a-a120-b270b68bd609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import from_json, col, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "# Read the table\n",
    "# df = spark.table(RAW_EXTRACTS)\n",
    "df = spark.table(RAW_EXTRACTS)\n",
    "\n",
    "# Define schema for the response column\n",
    "aspect_term_schema = StructType([\n",
    "    StructField(\"aspect\", StringType()),\n",
    "    StructField(\"evidence\", ArrayType(StringType())),\n",
    "    StructField(\"opinion_terms\", ArrayType(StringType())),\n",
    "    StructField(\"sentiment\", StringType())\n",
    "])\n",
    "\n",
    "response_schema = StructType([\n",
    "    StructField(\"aspect_terms\", ArrayType(aspect_term_schema))\n",
    "])\n",
    "\n",
    "\n",
    "# Parse the response column\n",
    "parsed_df = df.withColumn(\n",
    "    \"js\",\n",
    "    from_json(col(\"response\").cast(\"string\"), response_schema)\n",
    ")\n",
    "\n",
    "# Explode aspect_terms\n",
    "exploded_df = parsed_df.select(\n",
    "    \"review_id\",\n",
    "    \"review_text\",\n",
    "    explode(col(\"js.aspect_terms\")).alias(\"a\")\n",
    ").select(\n",
    "    \"review_id\",\n",
    "    \"review_text\",\n",
    "    col(\"a.aspect\").alias(\"aspect\"),\n",
    "    col(\"a.sentiment\").alias(\"sentiment\"),\n",
    "    col(\"a.evidence\").alias(\"evidence\"),\n",
    "    col(\"a.opinion_terms\").alias(\"opinion_terms\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1762ab14-3a66-4826-a467-8c3ff21232c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(exploded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "984273bb-9ad7-4f7d-96b2-dd6c859bb74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_reviews_df = spark.table(RAW_REVIEWS)\n",
    "\n",
    "exploded_df.count(), raw_reviews_df.count()\n",
    "display(raw_reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4534c2cd-53f8-44f0-bb4d-12ead3911f01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_reviews_df.toPandas()[[\"review_id\",'review_text']].drop_duplicates()\n",
    "d = exploded_df.join(raw_reviews_df[['location',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'review_id',\n",
    " 'channel',\n",
    " 'star_rating',\n",
    " 'review_date',\n",
    " 'review_text']], on=[\"review_id\", \"review_text\"], how=\"left\").toPandas()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289c8fb7-e164-4ec4-96b1-666848991a78",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760639378496}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the raw reviews table\n",
    "raw_reviews_df = spark.table(RAW_REVIEWS)\n",
    "\n",
    "# Merge the exploded aspects with raw reviews on the review identifier\n",
    "merged_df = exploded_df.join(raw_reviews_df[['location',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'review_id',\n",
    " 'channel',\n",
    " 'star_rating',\n",
    " 'review_date',\n",
    " 'review_text']], on=[\"review_id\", \"review_text\"], how=\"left\")\n",
    "\n",
    "# Show the merged result\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c8f411e-ffce-45b1-8364-2c9bb2e2805f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111ced4a-b7e2-4ee8-ade6-962308d0fa14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_df.write.mode(\"overwrite\").saveAsTable(RAW_ASPECTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3e2a27b-4936-4e7c-afad-716c0c577ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Daily Aggregated Issues by location_aspect\n",
    "\n",
    "- `lakehouse_inn_catalog.voc.loc_aspect_daily`\n",
    "| Column                  | Description                                                  |\n",
    "|--------------------------|--------------------------------------------------------------|\n",
    "| `location`               | City or hotel region                                         |\n",
    "| `aspect`                 | Review aspect (e.g., cleanliness, wifi_connectivity)         |\n",
    "| `date`                   | Review date                                                  |\n",
    "| `total_mentions`         | Total mentions that day                                      |\n",
    "| `neg_mentions`           | Count of negative + very_negative mentions                   |\n",
    "| `pos_mentions`           | Count of positive + very_positive mentions                   |\n",
    "| `neu_mentions`           | Count of neutral mentions                                    |\n",
    "| `neg_share`              | Daily negative share (`neg_mentions / total_mentions`)       |\n",
    "| `baseline_neg_share_21d` | Average negative share of past 21 days                       |\n",
    "| `neg_share_7d`           | 7-day average including current day                          |\n",
    "| `volume_7d`              | Total mentions over last 7 days                              |\n",
    "| `delta_pp`               | Change in negative share (percentage points) vs 21-day baseline |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd40740-aaf8-4854-97e9-2709bc959636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window as W\n",
    "\n",
    "daily = (\n",
    "    merged_df.withColumn(\"date\", F.col(\"review_date\").cast(\"date\"))\n",
    "    .groupBy(\"location\", \"aspect\", \"date\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_mentions\"),\n",
    "        F.sum(F.when(F.col(\"sentiment\").isin(\"negative\", \"very_negative\"), 1).otherwise(0)).alias(\"neg_mentions\"),\n",
    "        F.sum(F.when(F.col(\"sentiment\").isin(\"positive\", \"very_positive\"), 1).otherwise(0)).alias(\"pos_mentions\"),\n",
    "        F.sum(F.when(F.col(\"sentiment\") == \"neutral\", 1).otherwise(0)).alias(\"neu_mentions\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"neg_share\",\n",
    "        F.when(F.col(\"total_mentions\") > 0,\n",
    "               F.col(\"neg_mentions\").cast(\"double\") / F.col(\"total_mentions\").cast(\"double\"))\n",
    "         .otherwise(F.lit(None).cast(\"double\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4️⃣ Rolling window definitions\n",
    "w_prev21 = W.partitionBy(\"location\", \"aspect\").orderBy(\"date\").rowsBetween(-21, -1)  # baseline (exclude current)\n",
    "w_last7  = W.partitionBy(\"location\", \"aspect\").orderBy(\"date\").rowsBetween(-6, 0)   # short-term (include current)\n",
    "\n",
    "# 5️⃣ Compute rolling metrics\n",
    "final = (\n",
    "    daily\n",
    "    .withColumn(\"sum_neg_prev21\", F.sum(\"neg_mentions\").over(w_prev21))\n",
    "    .withColumn(\"sum_tot_prev21\", F.sum(\"total_mentions\").over(w_prev21))\n",
    "    .withColumn(\n",
    "        \"baseline_neg_share_21d\",\n",
    "        F.when(F.col(\"sum_tot_prev21\") > 0,\n",
    "               F.col(\"sum_neg_prev21\").cast(\"double\") / F.col(\"sum_tot_prev21\").cast(\"double\"))\n",
    "         .otherwise(F.lit(None).cast(\"double\"))\n",
    "    )\n",
    "    .withColumn(\"sum_neg_last7\", F.sum(\"neg_mentions\").over(w_last7))\n",
    "    .withColumn(\"sum_tot_last7\", F.sum(\"total_mentions\").over(w_last7))\n",
    "    .withColumn(\n",
    "        \"neg_share_7d\",\n",
    "        F.when(F.col(\"sum_tot_last7\") > 0,\n",
    "               F.col(\"sum_neg_last7\").cast(\"double\") / F.col(\"sum_tot_last7\").cast(\"double\"))\n",
    "         .otherwise(F.lit(None).cast(\"double\"))\n",
    "    )\n",
    "    .withColumn(\"volume_7d\", F.col(\"sum_tot_last7\").cast(\"double\"))\n",
    "    .withColumn(\"date_dt\", F.col(\"date\"))\n",
    "    .withColumn(\n",
    "        \"delta_pp\",\n",
    "        F.when(\n",
    "            F.col(\"baseline_neg_share_21d\").isNotNull() & F.col(\"neg_share_7d\").isNotNull(),\n",
    "            (F.col(\"neg_share_7d\") - F.col(\"baseline_neg_share_21d\")) * F.lit(100.0)\n",
    "        ).otherwise(F.lit(None).cast(\"double\"))\n",
    "    )\n",
    "    .select(\n",
    "        \"location\",\n",
    "        \"aspect\",\n",
    "        \"date\",\n",
    "        \"total_mentions\",\n",
    "        \"neg_mentions\",\n",
    "        \"pos_mentions\",\n",
    "        \"neu_mentions\",\n",
    "        \"neg_share\",\n",
    "        \"date_dt\",\n",
    "        \"baseline_neg_share_21d\",\n",
    "        \"neg_share_7d\",\n",
    "        \"volume_7d\",\n",
    "        \"delta_pp\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8ea127-980f-43f9-bc11-b875e05e5506",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760652801848}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final.write.mode(\"overwrite\").saveAsTable(ASPECTS_DAILY)\n",
    "display(spark.table(ASPECTS_DAILY).orderBy(\"location\", \"aspect\", \"date\").limit(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb26273f-8c8f-429f-9b2f-e17e998d5044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Flag Issues: issues_daily\n",
    "(do we need all reviews or negative reviews in here)\n",
    "### issues_daily Table Schema\n",
    "\n",
    "| **Column** | **Type** | **Description** |\n",
    "|-------------|-----------|-----------------|\n",
    "| `issue_id` | `STRING` | Unique issue identifier (e.g., `RUN02-0001`). |\n",
    "| `opened_at` | `DATE` | Date when the issue was detected (based on daily metrics). |\n",
    "| `location` | `STRING` | Hotel city or region where the issue occurred. |\n",
    "| `aspect` | `STRING` | Review aspect associated with the issue (e.g., `cleanliness`, `wifi_connectivity`). |\n",
    "| `severity` | `STRING` | Issue severity level (`Critical`, `Warning`) determined from thresholds on sentiment trends. |\n",
    "| `status` | `STRING` | Issue status (`Open`, `Closed`); default is `Open`. |\n",
    "| `open_reason` | `STRING` | Explanation of why the issue was opened (e.g., “Neg share 78% (+16.2pp vs 21d)”). |\n",
    "| `delta_pp_open` | `DOUBLE` | Change in negative share (percentage points) vs 21-day baseline. |\n",
    "| `nms_open` | `DOUBLE` | 7-day average negative share (normalized metric). |\n",
    "| `volume_open` | `BIGINT` | 7-day total number of mentions contributing to this issue. |\n",
    "| `relevant_reviews` | `ARRAY<STRUCT>` | Up to 5 most relevant reviews linked to the issue (negative-first, recent-first). Each struct contains: |\n",
    "| → `review_id` | `STRING` | Unique review identifier. |\n",
    "| → `sentiment` | `STRING` | Sentiment for this aspect (`very_negative` → `very_positive`). |\n",
    "| → `evidence` | `ARRAY<STRING>` | Extracted snippets supporting the sentiment classification. |\n",
    "| → `opinion_terms` | `ARRAY<STRING>` | Opinion keywords or phrases about the aspect (e.g., “dirty”, “outdated”). |\n",
    "| → `review_date` | `DATE` | Date of the review. |\n",
    "| → `review_text` | `STRING` | Full text of the customer review. |\n",
    "\n",
    "---\n",
    "\n",
    "####  Threshold Logic\n",
    "- **Critical:** `neg_share_7d ≥ 0.85` or `delta_pp ≥ 30.0`\n",
    "- **Warning:** `neg_share_7d ≥ 0.70` or `delta_pp ≥ 20.0`\n",
    "- **Minimum volume:** only triggered if `volume_7d ≥ 10`\n",
    "\n",
    "####  Threshold Logic (10/22) \n",
    "`delta_pp`Change in negative share (percentage points) vs 21-day baseline. \n",
    "- **Critical:** `neg_share_7d ≥ 0.85` or `delta_pp ≥ 15.0`\n",
    "- **Warning:** `neg_share_7d ≥ 0.70` or `delta_pp ≥ 10.0`\n",
    "- **Minimum volume:** only triggered if `volume_7d ≥ 10`\n",
    "\n",
    "#### Granularity\n",
    "- One row per `(location, aspect, date)` combination that meets or exceeds alert thresholds.  \n",
    "- Reviews are limited to the past **7 days** for the same `(location, aspect)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ac9dfc6-823b-4ad8-a1a5-d85fa8b9fe3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window as W\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Assumes ASPECTS_DAILY is already defined in your env.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# 1) Load daily metrics and classify severity\n",
    "df = spark.table(ASPECTS_DAILY)\n",
    "\n",
    "THRESHOLDS = {\n",
    "    \"critical_neg_share\": 0.85,  # >75% negative sentiment\n",
    "    \"critical_delta_pp\": 15.0,   # >15pp spike in negativity\n",
    "    \"warning_neg_share\": 0.70,   # >60% negative sentiment\n",
    "    \"warning_delta_pp\": 10.0,     # >8pp increase\n",
    "    \"min_volume\": 10             # only consider if volume >= 10 mentions\n",
    "}\n",
    "\n",
    "df_issues = (\n",
    "    df.filter(F.col(\"volume_7d\") >= THRESHOLDS[\"min_volume\"])\n",
    "      .withColumn(\n",
    "          \"severity\",\n",
    "          F.when(\n",
    "              (F.col(\"neg_share_7d\") >= THRESHOLDS[\"critical_neg_share\"]) |\n",
    "              (F.col(\"delta_pp\") >= THRESHOLDS[\"critical_delta_pp\"]),\n",
    "              F.lit(\"Critical\")\n",
    "          )\n",
    "          .when(\n",
    "              (F.col(\"neg_share_7d\") >= THRESHOLDS[\"warning_neg_share\"]) |\n",
    "              (F.col(\"delta_pp\") >= THRESHOLDS[\"warning_delta_pp\"]),\n",
    "              F.lit(\"Warning\")\n",
    "          )\n",
    "          .otherwise(F.lit(None))\n",
    "      )\n",
    "      .filter(F.col(\"severity\").isNotNull())\n",
    ")\n",
    "\n",
    "\n",
    "df_final = (\n",
    "    df_issues\n",
    "    .withColumn(\n",
    "        \"issue_id\",\n",
    "        F.concat(\n",
    "            F.lit(\"RUN-\"),\n",
    "            F.date_format(F.lit(current_date_param), \"yyyyMMdd\"),\n",
    "            F.lit(\"-\"),\n",
    "            F.lpad((F.monotonically_increasing_id() % 1_000_000).cast(\"string\"), 6, \"0\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"opened_at\", F.col(\"date\"))\n",
    "    .withColumn(\n",
    "        \"status\",\n",
    "        F.when(\n",
    "            F.col(\"date\") < F.add_months(F.lit(current_date_param), -1),\n",
    "            F.lit(\"Closed\")\n",
    "        ).otherwise(F.lit(\"Open\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"open_reason\",\n",
    "        F.concat(\n",
    "            F.lit(\"Neg share \"),\n",
    "            F.format_number(F.col(\"neg_share_7d\") * 100, 1),\n",
    "            F.lit(\"% (+\"),\n",
    "            F.format_number(F.col(\"delta_pp\"), 1),\n",
    "            F.lit(\"pp vs 21d)\")\n",
    "        )\n",
    "    )\n",
    "    .select(\n",
    "        \"issue_id\",\n",
    "        \"opened_at\",\n",
    "        \"location\",\n",
    "        \"aspect\",\n",
    "        \"severity\",\n",
    "        \"status\",\n",
    "        \"open_reason\",\n",
    "        F.round(\"delta_pp\", 1).alias(\"delta_pp_open\"),\n",
    "        F.round(\"neg_share_7d\", 3).alias(\"nms_open\"),\n",
    "        F.round(\"volume_7d\", 0).cast(\"bigint\").alias(\"volume_open\"),\n",
    "    )\n",
    ")\n",
    "# 2) Reviews: use review_aspect_details (already has all fields)\n",
    "RAW_ASPECTS = \"lakehouse_inn_catalog.voc.review_aspect_details\"\n",
    "\n",
    "review_aspects = spark.table(RAW_ASPECTS).select(\n",
    "    \"review_id\",\n",
    "    \"aspect\",\n",
    "    \"sentiment\",\n",
    "    \"evidence\",\n",
    "    \"opinion_terms\",\n",
    "    \"location\",\n",
    "    \"review_date\",\n",
    "    \"review_text\",\n",
    "    \"channel\",\n",
    "    \"star_rating\"\n",
    ")\n",
    "\n",
    "# 3) Join issues with reviews (matching location, aspect, date range)\n",
    "joined = (\n",
    "    df_final.alias(\"i\")\n",
    "    .join(\n",
    "        review_aspects.alias(\"x\"),\n",
    "        on=[F.col(\"i.location\") == F.col(\"x.location\"),\n",
    "            F.col(\"i.aspect\")   == F.col(\"x.aspect\")],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"x.review_date\") >= F.date_sub(F.col(\"i.opened_at\"), 6)) &\n",
    "        (F.col(\"x.review_date\") <= F.col(\"i.opened_at\"))\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"i.issue_id\").alias(\"issue_id\"),\n",
    "        F.col(\"i.opened_at\").alias(\"opened_at\"),\n",
    "        F.col(\"i.location\").alias(\"location\"),\n",
    "        F.col(\"i.aspect\").alias(\"aspect\"),\n",
    "        F.col(\"i.severity\").alias(\"severity\"),\n",
    "        F.col(\"i.status\").alias(\"status\"),\n",
    "        F.col(\"i.open_reason\").alias(\"open_reason\"),\n",
    "        F.col(\"i.delta_pp_open\").alias(\"delta_pp_open\"),\n",
    "        F.col(\"i.nms_open\").alias(\"nms_open\"),\n",
    "        F.col(\"i.volume_open\").alias(\"volume_open\"),\n",
    "        F.col(\"x.review_id\").alias(\"review_id\"),\n",
    "        F.col(\"x.sentiment\").alias(\"review_sentiment\"),\n",
    "        F.col(\"x.evidence\").alias(\"evidence\"),\n",
    "        F.col(\"x.opinion_terms\").alias(\"opinion_terms\"),\n",
    "        F.col(\"x.review_date\").alias(\"review_date\"),\n",
    "        F.col(\"x.review_text\").alias(\"review_text\"),\n",
    "        F.col(\"x.channel\").alias(\"channel\"),\n",
    "        F.col(\"x.star_rating\").alias(\"star_rating\"),\n",
    "    )\n",
    "    # # Deduplicate: one review per issue\n",
    "    # .dropDuplicates([\"issue_id\", \"review_id\"])\n",
    ")\n",
    "\n",
    "# ======================================================\n",
    "# Prioritize very_negative > negative > neutral > positive > very_positive, then most recent\n",
    "# ======================================================\n",
    "priority_map = F.create_map(\n",
    "    F.lit(\"very_negative\"), F.lit(5),\n",
    "    F.lit(\"negative\"),      F.lit(4),\n",
    "    F.lit(\"neutral\"),       F.lit(3),\n",
    "    F.lit(\"positive\"),      F.lit(2),\n",
    "    F.lit(\"very_positive\"), F.lit(1)\n",
    ")\n",
    "\n",
    "ranked = joined.withColumn(\"priority\", priority_map[F.col(\"review_sentiment\")])\n",
    "\n",
    "w = (\n",
    "    W.partitionBy(\"issue_id\")\n",
    "     .orderBy(\n",
    "         F.col(\"priority\").desc_nulls_last(),\n",
    "         F.col(\"review_date\").desc_nulls_last(),\n",
    "         F.col(\"review_id\")\n",
    "     )\n",
    ")\n",
    "\n",
    "# ======================================================\n",
    "# Collect top 5 reviews per issue\n",
    "# ======================================================\n",
    "# top_reviews = (\n",
    "#     ranked\n",
    "#     .withColumn(\"rn\", F.row_number().over(w))\n",
    "#     .filter(F.col(\"rn\") <= 100)\n",
    "#     .groupBy(\n",
    "#         \"issue_id\",\"opened_at\",\"location\",\"aspect\",\n",
    "#         \"severity\",\"status\",\"open_reason\",\n",
    "#         \"delta_pp_open\",\"nms_open\",\"volume_open\"\n",
    "#     )\n",
    "#     .agg(\n",
    "#         F.collect_list(\n",
    "#             F.struct(\n",
    "#                 F.col(\"review_id\"),\n",
    "#                 F.col(\"review_sentiment\").alias(\"sentiment\"),\n",
    "#                 F.col(\"evidence\"),\n",
    "#                 F.col(\"opinion_terms\"),\n",
    "#                 F.col(\"review_date\"),\n",
    "#                 F.col(\"review_text\"),\n",
    "#                 F.col(\"channel\"),\n",
    "#                 F.col(\"star_rating\")\n",
    "#             )\n",
    "#         ).alias(\"relevant_reviews\")\n",
    "#     )\n",
    "# )\n",
    "top_reviews = (\n",
    "    joined\n",
    "    .groupBy(\n",
    "        \"issue_id\",\"opened_at\",\"location\",\"aspect\",\n",
    "        \"severity\",\"status\",\"open_reason\",\n",
    "        \"delta_pp_open\",\"nms_open\",\"volume_open\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.struct(\n",
    "                F.col(\"review_id\"),\n",
    "                F.col(\"review_sentiment\").alias(\"sentiment\"),\n",
    "                F.col(\"evidence\"),\n",
    "                F.col(\"opinion_terms\"),\n",
    "                F.col(\"review_date\"),\n",
    "                F.col(\"review_text\"),\n",
    "                F.col(\"channel\"),\n",
    "                F.col(\"star_rating\")\n",
    "            )\n",
    "        ).alias(\"relevant_reviews\")\n",
    "    )\n",
    ")\n",
    "# ======================================================\n",
    "# Left join back to issues table (keep empty arrays)\n",
    "# ======================================================\n",
    "final_issues = (\n",
    "    df_final.alias(\"i\")\n",
    "    .join(\n",
    "        top_reviews.alias(\"t\"),\n",
    "        on=[\"issue_id\",\"opened_at\",\"location\",\"aspect\",\"severity\",\"status\",\n",
    "            \"open_reason\",\"delta_pp_open\",\"nms_open\",\"volume_open\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"relevant_reviews\",\n",
    "        F.coalesce(\n",
    "            F.col(\"relevant_reviews\"),\n",
    "            F.array().cast(\n",
    "                \"array<struct<review_id:string,sentiment:string,evidence:array<string>,opinion_terms:array<string>,review_date:date,review_text:string,channel:string,star_rating:bigint>>\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "display(final_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2894ae0-e4e6-4cfb-9205-3fa2de8bab8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_issues.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53a12efc-b19f-4ea2-9fa2-f8a7699d2ab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "mark some issues as closed (only keepiung the latest aspect-location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066faf19-afbc-4917-ad2b-2d38b637132a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Window to get the latest opened_at per (aspect, location)\n",
    "w = W.partitionBy(\"aspect\", \"location\").orderBy(F.col(\"opened_at\").desc())\n",
    "\n",
    "# Add row number to identify the latest issue per group\n",
    "issues_ranked = final_issues.withColumn(\"rn\", F.row_number().over(w))\n",
    "\n",
    "# Mark only the latest as Open, others as Closed\n",
    "issues_final = (\n",
    "    issues_ranked.withColumn(\n",
    "        \"status\",\n",
    "        F.when(F.col(\"rn\") == 1, F.lit(\"Open\")).otherwise(F.lit(\"Closed\"))\n",
    "    )\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "issues_final.filter(F.col(\"status\") == \"Open\").count()\n",
    "# display(issues_final.filter(F.col(\"status\") == \"Open\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6582b125-94ee-44c3-b039-9afbbcce1c62",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760822943767}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "issues_with_text = issues_final.withColumn(\n",
    "    \"relevant_reviews_text\",\n",
    "    F.expr(\"\"\"\n",
    "        concat(\n",
    "            'Aspect: ', aspect, '\\n\\n',\n",
    "            'Relevant Review Extracts: ', '\\n',\n",
    "            concat_ws(\n",
    "                '\\n\\n',\n",
    "                transform(\n",
    "                    relevant_reviews,\n",
    "                    x -> concat(\n",
    "                        'review_id:', x.review_id, \n",
    "                        '; sentiment:', x.sentiment, \n",
    "                        '; evidence:', array_join(x.evidence, '|'), \n",
    "                        '; opinion_terms:', array_join(x.opinion_terms, '|'), \n",
    "                        '; review_date:', cast(x.review_date as string), \n",
    "                        '; channel:', x.channel, \n",
    "                        '; star_rating:', cast(x.star_rating as string)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "display(issues_with_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b1c436-b9b1-4926-aa6e-59c455541ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "issues_with_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19257905-1c54-4cd4-a63a-79d477d17d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "issues_with_text.write.mode(\"overwrite\").saveAsTable(ISSUES_DAILY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e6f1782-16e1-4597-a61c-e9372e3ccd90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1bb1a3c-e913-43d9-a957-5bb275227ed7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760730763810}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Example: filter the final issues dataframe for Boston, MA\n",
    "boston_issues = issues_with_text.filter(F.col(\"location\") == \"Austin, TX\")\n",
    "\n",
    "display(boston_issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5c3e891-85b5-4dbe-99d5-a298f8ded7be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da515228-151f-4230-82b9-07e74c5424d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "duplicate_count = (\n",
    "    spark.table(ISSUES_DAILY).groupBy(\n",
    "        \"aspect\",\n",
    "        \"location\",\n",
    "        \"opened_at\"\n",
    "    )\n",
    "    .count()\n",
    "    .filter(\n",
    "        F.col(\"count\") > 1\n",
    "    )\n",
    "    .agg(\n",
    "        F.sum(\"count\").alias(\"duplicate_rows\")\n",
    "    )\n",
    "    .collect()[0][\"duplicate_rows\"]\n",
    ")\n",
    "\n",
    "duplicate_count = duplicate_count if duplicate_count is not None else 0\n",
    "duplicate_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69383e7d-6428-4512-8260-8d1c59de6ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ISSUES_DAILY = \"lakehouse_inn_catalog.voc.issues_daily\"\n",
    "row_count = spark.table(ISSUES_DAILY).count()\n",
    "row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f468175-4f6a-4c35-884b-08901045a761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next Step: Diagnosis and Recommendations for each issue\n",
    "-> Agent Bricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2c2a3a9-8300-4d2f-855d-b66d9cb7a383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61142754-361f-41f3-98b3-c7b6e464e7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8603944c-0903-4c29-b7f8-ebf47f781c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Map sentiment to numerical severity weight\n",
    "sentiment_weights = {\n",
    "    \"very_positive\": -2,\n",
    "    \"positive\": -1,\n",
    "    \"neutral\": 0,\n",
    "    \"negative\": 1,\n",
    "    \"very_negative\": 2\n",
    "}\n",
    "\n",
    "sentiment_expr = F.create_map([F.lit(x) for kv in sentiment_weights.items() for x in kv])\n",
    "\n",
    "# Assign numeric weight per aspect\n",
    "df_weighted = merged_df.withColumn(\"severity_weight\", sentiment_expr[F.col(\"sentiment\")])\n",
    "\n",
    "# Focus only on negative aspects\n",
    "issues_df = df_weighted.filter(F.col(\"severity_weight\") > 0)\n",
    "\n",
    "# Compute issue frequency and severity per aspect\n",
    "issue_summary = (\n",
    "    issues_df.groupBy(\"location\", \"aspect\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"review_id\").alias(\"issue_count\"),\n",
    "        F.avg(\"severity_weight\").alias(\"avg_severity\"),\n",
    "    )\n",
    "    .withColumn(\"severity_score\", F.col(\"issue_count\") * F.col(\"avg_severity\"))\n",
    "    .orderBy(F.desc(\"severity_score\"))\n",
    ")\n",
    "\n",
    "display(issue_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33401d06-6a95-48d7-adb5-7e8b0dbc8dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Write to table\n",
    "exploded_df.write.mode(\"overwrite\").saveAsTable(\"lakehouse_inn_catalog.voc.issue_summary\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8087660966816293,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2_Issue_Idenitification",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
